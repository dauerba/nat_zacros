{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39f1074",
   "metadata": {},
   "source": [
    "# nat-Zarcos Analysis Workflow -- v1\n",
    "\n",
    "\n",
    "This notebook demonstrates the usage of the nat-zacros package for analyzing Zacros simulation data.\n",
    "\n",
    "(Note: formerly named zacros_analysis_v1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c54b6",
   "metadata": {},
   "source": [
    "## 0. Debug Automatic Versioning Snipits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c8feb",
   "metadata": {},
   "source": [
    "### find locations of sitepackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import site; print(site.getsitepackages())\n",
    "# import site; print(site.getusersitepackages())\n",
    "import site\n",
    "import os\n",
    "\n",
    "site_packages = site.getsitepackages() + [site.getusersitepackages()]\n",
    "for sp in site_packages:\n",
    "    print(f\"\\nListing in: {sp}\")\n",
    "    if os.path.isdir(sp):\n",
    "        for name in os.listdir(sp):\n",
    "            if \"nat_zacros\" in name or \"nat-zacros\" in name or \"egg-link\" in name:\n",
    "                print(\"  \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nat_zacros\n",
    "importlib.reload(nat_zacros)\n",
    "print(nat_zacros.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2320d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nat_zacros\n",
    "print(nat_zacros.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5396d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"GIT DIR exists:\", os.path.isdir(os.path.join(os.getcwd(), \".git\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"GIT DIR exists:\", os.path.isdir(os.path.join(os.getcwd(), \".git\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools_scm\n",
    "print(setuptools_scm.get_version(root=\"c:/Users/a-DJA/GIT/nat_zacros\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nat_zacros\n",
    "importlib.reload(nat_zacros)\n",
    "print(nat_zacros.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847a63f",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665a088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System  : Windows\n",
      "User              : a-DJA\n",
      "nat_zacros version: 0.0.7.dev4+g63ceb25a0\n",
      "Data directory    : c:/Users/a-DJA/Dropbox/Surface_Reaction_Kinetics/O_Pt111/zacros_calculations\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ------            This code no longer needed                           ------\n",
    "# ------            use environment with nat-zarcos installed            ------\n",
    "# -----------------------------------------------------------------------------\n",
    "# # Add nat_zacros package to path (located in ../nat_zacros relative to this notebook)\n",
    "# project_root = Path.cwd().parent  # Goes from O_Pt111 to GIT\n",
    "# nat_zacros_pkg_dir = project_root / 'nat_zacros'\n",
    "# if nat_zacros_pkg_dir.exists() and str(nat_zacros_pkg_dir) not in sys.path:\n",
    "#     sys.path.insert(0, str(nat_zacros_pkg_dir))\n",
    "#     print(f\"Added to path: {nat_zacros_pkg_dir}\")\n",
    "\n",
    "import importlib\n",
    "import nat_zacros\n",
    "# importlib.reload(nat_zacros)\n",
    "\n",
    "from nat_zacros import lattice, state, trajectory, load_trajectories_parallel\n",
    "\n",
    "# Dictionary mapping usernames to data paths\n",
    "user_paths = {\n",
    "    'a-DJA'  : Path('c:/Users/a-DJA/Dropbox/Surface_Reaction_Kinetics/O_Pt111/zacros_calculations'),\n",
    "    'akandra': Path('/home/akandra/O_Pt111/zacros_calculations'),\n",
    "}\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    username = os.getenv('USERNAME')\n",
    "else:  # Linux/Mac\n",
    "    username = os.getenv('USER')\n",
    "\n",
    "# Get path for current user\n",
    "if username in user_paths:\n",
    "    dir_with_calculations = user_paths[username]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown user: {username}. Add your path to user_paths dictionary.\")\n",
    "\n",
    "print(f\"Operating System  : {platform.system()}\")\n",
    "print(f\"User              : {username}\")\n",
    "print(f\"nat_zacros version: {nat_zacros.__version__}\")\n",
    "print(f\"Data directory    : {dir_with_calculations.as_posix()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1722ce",
   "metadata": {},
   "source": [
    "## 2. Specify directories for a group of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "log_dir = dir_with_calculations / 'fn_3leed'\n",
    "data_dir = log_dir / 'jobs'\n",
    "results_dir = log_dir / 'results'\n",
    "log_file = data_dir / 'jobs.log'\n",
    "\n",
    "# Create results directory\n",
    "Path.mkdir(results_dir, exist_ok=True)\n",
    "\n",
    "# Verify the file exists\n",
    "if not log_file.exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {log_file}\")\n",
    "\n",
    "# Load json log file\n",
    "with open(log_file, 'r') as f:\n",
    "    log_header = f.readline().split()  # Read header\n",
    "    log_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Parse job information\n",
    "run_dirs = [data_dir / str(entry[0]) for entry in log_entries]\n",
    "temperatures = [entry[4] for entry in log_entries]\n",
    "lat_size = [entry[2][0]*entry[2][1] for entry in log_entries]\n",
    "n_ads = [entry[3][0] for entry in log_entries]\n",
    "interactions = ['-'.join(entry[5][1:]) for entry in log_entries]\n",
    "coverages = [n_ads[i]/lat_size[i] for i in range(len(n_ads))]\n",
    "print(f\"Found {len(run_dirs)} simulation runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31933b96",
   "metadata": {},
   "source": [
    "## 3. Determine Equilibration Time\n",
    "\n",
    "**Purpose:** Load full trajectories and plot energy vs time to determine equilibration.\n",
    "\n",
    "**Workflow:** \n",
    "1. Load ALL trajectory data (no equilibration cutoff)\n",
    "2. Compute average energy vs time\n",
    "3. Plot and visually inspect to identify equilibration point\n",
    "4. Use this to set the `fraction` parameter in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select run for analysis\n",
    "run_number = 0  \n",
    "rundir = run_dirs[run_number]\n",
    "print(f\"Analyzing run : {rundir.name}\")\n",
    "print(f\"  Temperature : {temperatures[run_number]} K\")\n",
    "print(f\"  Coverage    : {coverages[run_number]:.3f} ML\")\n",
    "print(f\"  Interactions: {interactions[run_number]}\")\n",
    "\n",
    "# Find all trajectory directories\n",
    "traj_dirs = sorted([d for d in rundir.iterdir() if d.is_dir() and d.name.startswith('traj_')])\n",
    "print(f\"Found {len(traj_dirs)} trajectories\")\n",
    "\n",
    "# Start timing\n",
    "t_start = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading FULL trajectories with parallel I/O (all data, no cutoff)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create lattice (same for all trajectories)\n",
    "lat = lattice(traj_dirs[0])\n",
    "\n",
    "# Load ALL trajectory data (fraction=1.0 means no equilibration cutoff)\n",
    "trajs_full = load_trajectories_parallel(lat, traj_dirs, fraction=1.0, n_workers=None)\n",
    "\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Loading completed in {t_elapsed:.2f} seconds\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTrajectory info:\")\n",
    "print(f\"  Number of trajectories: {len(trajs_full)}\")\n",
    "print(f\"  States per trajectory: {len(trajs_full[0])} (full, not equilibrated)\")\n",
    "print(f\"  Total states: {sum(len(t) for t in trajs_full)}\")\n",
    "\n",
    "# Extract energy data and plot\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Computing average energy vs time from FULL trajectories\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_time_bins = 100\n",
    "\n",
    "# Use minimum end time across all trajectories\n",
    "end_time = min([traj.times[-1] for traj in trajs_full])\n",
    "print(f\"Common time range: 0 to {end_time:.2e} s\")\n",
    "\n",
    "# Create time bins starting from 0 (full trajectory)\n",
    "time_bins = np.linspace(0, end_time, n_time_bins + 1)\n",
    "time_bin_centers = 0.5 * (time_bins[:-1] + time_bins[1:])\n",
    "energy_hist = np.zeros(n_time_bins)\n",
    "\n",
    "# Accumulate energy from all FULL trajectories\n",
    "for traj in trajs_full:\n",
    "    times, energies = traj.get_energy_vs_time()\n",
    "    \n",
    "    # Bin the energy values\n",
    "    for t, energy in zip(times, energies):\n",
    "        if t <= end_time:\n",
    "            bin_index = np.digitize(t, time_bins, right=False) - 1\n",
    "            if 0 <= bin_index < n_time_bins:\n",
    "                energy_hist[bin_index] += energy\n",
    "\n",
    "# Average over trajectories\n",
    "energy_hist /= len(trajs_full)\n",
    "\n",
    "print(f\"Binned energy data from {len(trajs_full)} trajectories\")\n",
    "\n",
    "# Skip first few bins (initialization artifacts)\n",
    "skip_bins = 5\n",
    "\n",
    "print(f\"\\nEnergy statistics (after skipping first {skip_bins} bins):\")\n",
    "print(f\"  Mean:   {np.mean(energy_hist[skip_bins:]):.4f} eV\")\n",
    "print(f\"  Std:    {np.std(energy_hist[skip_bins:]):.4f} eV\")\n",
    "print(f\"  Min:    {np.min(energy_hist[skip_bins:]):.4f} eV\")\n",
    "print(f\"  Max:    {np.max(energy_hist[skip_bins:]):.4f} eV\")\n",
    "\n",
    "# Plot energy vs time\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(time_bin_centers[skip_bins:], energy_hist[skip_bins:], \n",
    "        marker='o', linestyle='-', markersize=3, linewidth=1.5, color='blue')\n",
    "ax.set_xlabel('Time (s)', fontsize=12)\n",
    "ax.set_ylabel('Energy (eV)', fontsize=12)\n",
    "ax.set_title(f'Energy vs Time - Full Trajectories\\n{interactions[run_number]}, T={temperatures[run_number]} K, θ={coverages[run_number]:.3f}',\n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estimate equilibration point\n",
    "eq_fraction = 0.5\n",
    "eq_bin = int(eq_fraction * n_time_bins)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INSPECT THE PLOT ABOVE TO DETERMINE EQUILIBRATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nVisual inspection guidance:\")\n",
    "print(\"  - Look for where energy stabilizes (becomes roughly constant)\")\n",
    "print(\"  - Initial transient = equilibration period\")\n",
    "print(\"  - Choose a fraction to discard (e.g., 0.5 = discard first 50%)\")\n",
    "print(\"\\nTypical values:\")\n",
    "print(\"  fraction=0.3 → keep last 30% (long equilibration)\")\n",
    "print(\"  fraction=0.5 → keep last 50% (moderate equilibration)\")\n",
    "print(\"  fraction=0.7 → keep last 70% (short equilibration)\")\n",
    "\n",
    "print(f\"\\nEnergy statistics (after {eq_fraction*100:.0f}% equilibration):\")\n",
    "print(f\"  Mean:   {np.mean(energy_hist[eq_bin:]):.4f} eV\")\n",
    "print(f\"  Std:    {np.std(energy_hist[eq_bin:]):.4f} eV\")\n",
    "print(f\"  Min:    {np.min(energy_hist[eq_bin:]):.4f} eV\")\n",
    "print(f\"  Max:    {np.max(energy_hist[eq_bin:]):.4f} eV\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac4d03",
   "metadata": {},
   "source": [
    "## 4. Load Equilibrated Trajectories with Caching\n",
    "\n",
    "This cell loads trajectories with equilibration cutoff and demonstrates caching:\n",
    "\n",
    "1. **Check for cached data** (binary pickle file)\n",
    "2. **If not cached**: Load using **parallel I/O** with equilibration fraction\n",
    "3. **Save to cache** for future use\n",
    "\n",
    "**Performance:**\n",
    "- First run: ~6-10s (parallel loading)\n",
    "- Subsequent runs: ~0.5s (cache loading)\n",
    "\n",
    "**Important:** Set the `fraction` parameter based on the equilibration plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache = False   # Set to True to force reload\n",
    "\n",
    "# Use same run_number as above\n",
    "print(f\"Loading equilibrated trajectories for run: {rundir.name}\")\n",
    "\n",
    "# Cache files for this run\n",
    "cache_file = results_dir / f'{rundir.name}_trajs_eq.pkl'\n",
    "gref_cache_file = results_dir / f'{rundir.name}_gref.pkl'\n",
    "\n",
    "# Clear cache if requested\n",
    "if clear_cache:\n",
    "    if cache_file.exists():\n",
    "        cache_file.unlink()\n",
    "        print(f\"Trajectory cache cleared: {cache_file.name}\")\n",
    "    if gref_cache_file.exists():\n",
    "        gref_cache_file.unlink()\n",
    "        print(f\"g_ref cache cleared: {gref_cache_file.name}\")\n",
    "\n",
    "print(f\"\\nTrajectory cache: {cache_file.name}\")\n",
    "print(f\"  Exists: {cache_file.exists()}\")\n",
    "print(f\"g_ref cache: {gref_cache_file.name}\")\n",
    "print(f\"  Exists: {gref_cache_file.exists()}\")\n",
    "\n",
    "# If cache exists, show its age and size\n",
    "if cache_file.exists():\n",
    "    import datetime\n",
    "    mod_time = datetime.datetime.fromtimestamp(cache_file.stat().st_mtime)\n",
    "    size_mb = cache_file.stat().st_size / 1024**2\n",
    "    print(f\"\\nTrajectory cache info:\")\n",
    "    print(f\"  Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "\n",
    "# Start timing\n",
    "t_start = time.time()\n",
    "\n",
    "if not cache_file.exists():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FIRST RUN: Loading trajectories with parallel I/O\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create lattice (already created above, but recreate for standalone execution)\n",
    "    lat = lattice(traj_dirs[0])\n",
    "    \n",
    "    # Load trajectories in parallel with equilibration cutoff\n",
    "    # ADJUST fraction based on equilibration plot above\n",
    "    eq_fraction = 0.5  # Keep last 50% of trajectory\n",
    "    print(f\"Using equilibration fraction: {eq_fraction} (keeping last {eq_fraction*100:.0f}%)\")\n",
    "    trajs = load_trajectories_parallel(lat, traj_dirs, fraction=eq_fraction, n_workers=None)\n",
    "    \n",
    "    # Save trajectories to cache\n",
    "    print(f\"\\nSaving trajectories to cache: {cache_file.name}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(trajs, f)\n",
    "    print(f\"Trajectory cache saved: {cache_file.stat().st_size / 1024**2:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CACHED: Loading trajectories from pickle file\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load from cache\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        trajs = pickle.load(f)\n",
    "    print(f\"Loaded {len(trajs)} trajectories from cache\")\n",
    "\n",
    "t_load = time.time() - t_start\n",
    "\n",
    "# =========================================================================\n",
    "# Compute and cache g_ref (reference RDF for normalization)\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Computing reference RDF (g_ref) for lattice normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# RDF parameters (must match those used in Section 5)\n",
    "r_max = 40.0  # Angstrom\n",
    "dr = 0.1  # Bin width\n",
    "\n",
    "if not gref_cache_file.exists():\n",
    "    print(\"Computing g_ref (one-time calculation per lattice)...\")\n",
    "    t_gref_start = time.time()\n",
    "    r_ref, g_ref = trajs[0].get_g_ref(r_max=r_max, dr=dr)\n",
    "    t_gref = time.time() - t_gref_start\n",
    "    \n",
    "    print(f\"g_ref computed in {t_gref:.2f} seconds\")\n",
    "    print(f\"  Array size: {len(g_ref)} bins\")\n",
    "    print(f\"  Distance range: 0 to {r_max:.1f} Å\")\n",
    "    \n",
    "    # Save g_ref to cache\n",
    "    print(f\"\\nSaving g_ref to cache: {gref_cache_file.name}\")\n",
    "    with open(gref_cache_file, 'wb') as f:\n",
    "        pickle.dump((r_ref, g_ref), f)\n",
    "    print(f\"g_ref cache saved: {gref_cache_file.stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"Loading g_ref from cache...\")\n",
    "    t_gref_start = time.time()\n",
    "    with open(gref_cache_file, 'rb') as f:\n",
    "        r_ref, g_ref = pickle.load(f)\n",
    "    t_gref = time.time() - t_gref_start\n",
    "    print(f\"g_ref loaded from cache in {t_gref:.2f} seconds\")\n",
    "    print(f\"  Array size: {len(g_ref)} bins\")\n",
    "\n",
    "t_total = t_load + t_gref\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LOADING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Trajectories:  {t_load:6.2f} s\")\n",
    "print(f\"g_ref:         {t_gref:6.2f} s\")\n",
    "print(f\"Total:         {t_total:6.2f} s\")\n",
    "print(f\"\\nTrajectory info:\")\n",
    "print(f\"  Number of trajectories: {len(trajs)}\")\n",
    "print(f\"  States per trajectory: {len(trajs[0])} (equilibrated)\")\n",
    "print(f\"  Total states: {sum(len(t) for t in trajs)}\")\n",
    "\n",
    "# Check if this looks like full or equilibrated data\n",
    "if 'trajs_full' in locals():\n",
    "    ratio = len(trajs[0]) / len(trajs_full[0])\n",
    "    print(f\"  Equilibration ratio: {ratio:.1%} of full trajectory\")\n",
    "    if ratio > 0.8:\n",
    "        print(f\"  ⚠️  WARNING: Cache may contain nearly full trajectories!\")\n",
    "        print(f\"     Consider setting clear_cache=True and rerunning\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453fc21",
   "metadata": {},
   "source": [
    "## 5. Radial Distribution Function (RDF) Analysis\n",
    "\n",
    "Compute RDF using **vectorized distance calculations** (OPTIMIZATION #1).\n",
    "\n",
    "**Performance note:** For 10 trajectories, sequential computation with vectorization is fastest (~2s).  \n",
    "Parallel RDF would be slower due to overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befbc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDF parameters (must match those in Section 4)\n",
    "lattice_constant = 2.821135  # 1nn distance in Angstroms (Pt(111))\n",
    "r_max = 40.0  # Angstrom\n",
    "dr = 0.1  # Bin width\n",
    "\n",
    "# g_ref was already computed and cached in Section 4\n",
    "print(\"=\" * 70)\n",
    "print(\"Computing RDF for all trajectories\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Using cached g_ref from Section 4\")\n",
    "print(f\"Total states to process: {sum(len(t) for t in trajs)}\")\n",
    "print(f\"States per trajectory: {len(trajs[0])}\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "# Compute RDF for each trajectory using vectorized distances\n",
    "rdfs = []\n",
    "for i, traj in enumerate(trajs):\n",
    "    print(f\"  Computing RDF {i+1}/{len(trajs)} ({len(traj)} states)...\", end='\\r')\n",
    "    r, g = traj.get_rdf(r_max=r_max, dr=dr, g_ref=g_ref, vectorized=True)\n",
    "    rdfs.append(g)\n",
    "\n",
    "# Average RDFs across all trajectories\n",
    "g_avg = np.mean(rdfs, axis=0)\n",
    "g_std = np.std(rdfs, axis=0)\n",
    "\n",
    "# Convert distance to units of lattice constant\n",
    "r_a0 = r / lattice_constant\n",
    "\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"\\n\\n{'=' * 70}\")\n",
    "print(\"RDF COMPUTATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Time elapsed: {t_elapsed:.2f} seconds\")\n",
    "print(f\"  Trajectories: {len(trajs)}\")\n",
    "print(f\"  Total states: {sum(len(t) for t in trajs)}\")\n",
    "print(f\"  Average time per trajectory: {t_elapsed/len(trajs):.3f} s\")\n",
    "print(f\"  Average time per state: {t_elapsed/sum(len(t) for t in trajs)*1000:.2f} ms\")\n",
    "\n",
    "# Find peak information\n",
    "peak_idx = np.argmax(g_avg)\n",
    "print(f\"\\nRDF Peak Analysis:\")\n",
    "print(f\"  Peak position: {r_a0[peak_idx]:.2f} a₀ ({r[peak_idx]:.2f} Å)\")\n",
    "print(f\"  Peak height: {g_avg[peak_idx]:.3f}\")\n",
    "print(f\"  Note: No 1nn peak at 1.00 a₀ due to strong repulsive interaction\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RDF with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculate bar width based on bin spacing\n",
    "bar_width = (dr / lattice_constant) * 1.5\n",
    "\n",
    "# Plot bars with error bars\n",
    "ax.bar(r_a0, g_avg, width=bar_width, \n",
    "       ec='blue', fc='lightblue', alpha=0.7,\n",
    "       label=f'Average ({len(trajs)} traj)', align='center')\n",
    "ax.errorbar(r_a0, g_avg, yerr=g_std, \n",
    "           fmt='none', ecolor='black', capsize=0, \n",
    "           linewidth=1, alpha=0.6, zorder=10)\n",
    "\n",
    "# Reference line for ideal gas\n",
    "ax.axhline(1.0, color='k', linestyle='--', linewidth=1.5, \n",
    "          alpha=0.5, label='Ideal gas (g=1)')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(r'Distance ($r / a_0$)', fontsize=12)\n",
    "ax.set_ylabel('g(r)', fontsize=12)\n",
    "ax.set_title(f'Radial Distribution Function\\n{interactions[0]}, T={temperatures[0]} K, θ={coverages[0]:.3f}',\n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_xlim(0, r_max/lattice_constant)\n",
    "ax.set_ylim(0, max(g_avg) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10373c2a",
   "metadata": {},
   "source": [
    "## 6. Performance Summary\n",
    "\n",
    "This notebook demonstrates the **recommended workflow** for Zacros analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a44af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE OPTIMIZATIONS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"✅ APPLIED OPTIMIZATIONS:\")\n",
    "print()\n",
    "print(\"1. Vectorized Distance Calculations (50-100x speedup)\")\n",
    "print(\"   - Uses NumPy broadcasting for pairwise distances\")\n",
    "print(\"   - RDF computation: ~2s for 10 trajectories\")\n",
    "print(\"   - Enabled by: traj.get_rdf(vectorized=True)\")\n",
    "print()\n",
    "print(\"2. Parallel Loading (5-10x speedup)\")\n",
    "print(\"   - Multiple trajectories loaded simultaneously\")\n",
    "print(\"   - Loading time: ~6-10s (vs ~60s sequential)\")\n",
    "print(\"   - Enabled by: load_trajectories_parallel()\")\n",
    "print()\n",
    "print(\"3. Binary Caching (100x speedup for repeated analysis)\")\n",
    "print(\"   - Saves parsed trajectories to pickle file\")\n",
    "print(\"   - Subsequent loads: ~0.5s (vs ~60s parsing)\")\n",
    "print(\"   - Enabled by: pickle.dump() / pickle.load()\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"TYPICAL PERFORMANCE (10 trajectories, 14 cores):\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"First run (no cache):\")\n",
    "print(\"  Loading:        6-10 s  (parallel)\")\n",
    "print(\"  RDF computation: ~2 s   (vectorized sequential)\")\n",
    "print(\"  Total:          8-12 s\")\n",
    "print()\n",
    "print(\"Subsequent runs (with cache):\")\n",
    "print(\"  Loading:        0.5 s   (pickle cache)\")\n",
    "print(\"  RDF computation: ~2 s   (vectorized sequential)\")\n",
    "print(\"  Total:          2.5 s\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"NOT USED (counterproductive for typical datasets):\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"❌ Parallel RDF computation\")\n",
    "print(\"   - Overhead (2-4s) exceeds computation time (2s)\")\n",
    "print(\"   - Only beneficial for >50 trajectories\")\n",
    "print(\"   - Sequential is FASTER for typical use\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4c509",
   "metadata": {},
   "source": [
    "## 7. Usage Notes\n",
    "\n",
    "**Workflow summary:**\n",
    "1. **Section 3**: Load full trajectories and plot energy to determine equilibration\n",
    "2. **Section 4**: Load equilibrated trajectories (set `fraction` based on Section 3 plot)\n",
    "3. **Section 5**: Perform RDF or other analyses on equilibrated data\n",
    "\n",
    "**Best practices demonstrated:**\n",
    "1. Always check for cached data before loading\n",
    "2. Use parallel loading for I/O operations\n",
    "3. Use vectorized distance calculations (default in get_rdf)\n",
    "4. Use sequential loops for RDF computation (parallel has too much overhead)\n",
    "\n",
    "**To analyze a different run:**\n",
    "- Change `run_number` in Section 3\n",
    "- The cache file will be automatically created for the new run\n",
    "\n",
    "**To clear cache and force reload:**\n",
    "```python\n",
    "cache_file.unlink()  # Delete cache file\n",
    "# Or set clear_cache = True in Section 4\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dja1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
